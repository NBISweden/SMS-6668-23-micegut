{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a844d7-6631-4dde-a3d5-27ebe12dda5b",
   "metadata": {},
   "source": [
    "# NBIS support project 6668"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d050f84-8d65-4ba8-acb9-3e6ba8614d61",
   "metadata": {},
   "source": [
    "Shotgun metagenomic sequencing - Three-generations microbiome study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d03601-7ead-457a-9098-dc694fc53276",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "This project aims to study the effects of dibutyl phthalate (a plastic-derived contaminant) on the fecal microbiome composition and functional profile in three generations of mice (F0 \"exposed mice\" and F1 and F2 \"offsprings\"). Shotgun metagenomic sequencing of the DNA samples was done at NGI, Stockholm. The project has been discussed with John Sundh previously.\n",
    "\n",
    "Specific goals include:\n",
    "\n",
    "- To identify the taxonomic composition and profiling (species level), assess community diversity and characterize the relative abundances of taxa between the phthalates-treated mice vs. untreated mice (controls)\n",
    "- Deep functional characterization of the phthalate-treated mice and control mice focusing on antibiotic resistance genes, virulence factors, carbohydrate metabolism, functional redundancy, etc\n",
    "- To perform correlation analysis with other findings observed in F0, F1 and F2 mice, including immune, metabolic and liver phenotypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0424d94-e339-4ecb-b161-e0b4d2989751",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f57509-25d7-4a35-9af9-d3789005084b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34d4b3-5f6d-4b6b-86bb-ab5a9d390ba8",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22537d-f69d-4f2d-a417-a5f9eee9060b",
   "metadata": {},
   "source": [
    "Raw data is stored on Uppmax at:\n",
    "\n",
    "`/proj/snic2020-5-486/dbp_gut_microbiome/DataDelivery_2023-01-10_15-17-23_ngisthlm00104/files/P27457/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674e6c0-ed37-4893-9892-55acdbab6027",
   "metadata": {},
   "source": [
    "### Sample file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2368e-2697-4003-b620-fc6b498d71a2",
   "metadata": {},
   "source": [
    "A total of 108 samples were divided into 10 assembly groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aef76bc-f7ca-48c9-9f5b-e5ada42a06ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F0_C</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F0_H</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F0_L</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_C</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_H</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_L</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2_C</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2_H</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2_L</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mock</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       samples\n",
       "group         \n",
       "F0_C        12\n",
       "F0_H        12\n",
       "F0_L        10\n",
       "F1_C         8\n",
       "F1_H         8\n",
       "F1_L         6\n",
       "F2_C        16\n",
       "F2_H        16\n",
       "F2_L        17\n",
       "mock         3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv(\"../data/sample_list.csv\", header=0)\n",
    "sample_df.sort_values(\"group\", inplace=True)\n",
    "sample_df.to_csv(\"../data/sample_list.csv\", index=False)\n",
    "group_df = pd.DataFrame(sample_df.groupby(\"group\").size(), columns=[\"samples\"]).sort_index()\n",
    "group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a58682f-5680-41a2-9050-55fdb3c61229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for group in sample_df[\"group\"].unique():\n",
    "    #_df = sample_df.copy()\n",
    "    # Create one file with only samples from this group (for assembly)\n",
    "    #group_only = sample_df.loc[sample_df[\"group\"]==group]\n",
    "    # Create another file where all samples appear to be assigned to this group\n",
    "    #_df[\"group\"] = [group]*_df.shape[0]\n",
    "    #group_only.to_csv(f\"../data/sample_list.{group}-assemble.csv\", sep=\",\", index=False)\n",
    "    #_df.to_csv(f\"../data/sample_list.{group}-binning.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed512a-b4a6-47a5-bd29-81b4085b7d71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setting up nf-core/mag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff62d0-b843-4dc0-b774-32a8aede61a7",
   "metadata": {},
   "source": [
    "I installed a conda environment specifically to use with nf-core/mag using:\n",
    "\n",
    "```bash\n",
    "export CONDARC=\"/proj/snic2020-5-486/nobackup/SMS-23-6668-micegut/.condarc\"\n",
    "mamba env create -f mag-env.yml -p envs/mag\n",
    "mamba activate envs/mag\n",
    "# Copy scripts to set and unset environment variables\n",
    "mkdir -p $CONDA_PREFIX/etc/conda/deactivate.d $CONDA_PREFIX/etc/conda/activate.d\n",
    "cp src/activate.d/env_vars.sh $CONDA_PREFIX/etc/conda/activate.d/\n",
    "cp src/deactivate.d/env_vars.sh $CONDA_PREFIX/etc/conda/deactivate.d/\n",
    "# Re-activate the environment\n",
    "mamba activate envs/mag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65833744-f1a6-4213-a739-150c6ded6c96",
   "metadata": {},
   "source": [
    "#### Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46d7d3-18ec-4b22-a0a0-ecaa428ea144",
   "metadata": {},
   "source": [
    "There are some issues with the nf-core/mag pipeline that become apparent when trying to use on a large dataset with several co-assemblies. Using the setup specified here with 108 samples divided into 10 assembly groups and specifying `--binning_map_mode all` results in 108 x 10 = 1080 bam files. With each bam file ~Â 3-4 G in size that comes to 3.2 - 4.2 TB of data.\n",
    "\n",
    "If assemblies could be created and binned one at a time that would allow temporary bam files to be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e7947-091c-4a8d-a51c-f3fd6b33bfd7",
   "metadata": {},
   "source": [
    "##### Strategy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a086cfb-5c3f-41fc-a56e-c56db0937dad",
   "metadata": {},
   "source": [
    "First I tried to run the full pipeline as a node job after a tip from Phil Ewels. In theory that would allow the pipeline to use local node storage for the work directory, after which only the finished results could be copied to the project folder. \n",
    "\n",
    "However this resulted in failed runs with no apparent output in either the nextflow log or slurm logs. Also there are obvious caveats to this as the work directory cannot be saved in intermediate states if the pipeline fails (which in my experience is **always** the case at least once)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1fa54-da52-47b7-a405-2420dfa8a7e8",
   "metadata": {},
   "source": [
    "##### Strategy 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027292c-96f4-4571-9626-f2c0697e7161",
   "metadata": {},
   "source": [
    "Instead I tried to use group-specific input files where there'd be one sample list per assembly group. For example the 'mock' sample list would contain all samples but there would only be group assignments for the 'mock' group. The plan was to run the pipeline 10 times, once for each sample list. \n",
    "\n",
    "However, it appears the pipeline tries to create an assembly also for the unassigned group which obviously fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe6b64-6474-49c0-be44-a8952edcd866",
   "metadata": {},
   "source": [
    "##### Strategy 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ba488-4be1-46cb-9649-c0ee250cafa7",
   "metadata": {},
   "source": [
    "Next strategy is to try and first generate the co-assemblies using a params file where `--skip_binning` is set to true. That would run only the QC + Assembly steps which would only generate ~350 G of storage for the QCd reads + a few additional G for each assembly.\n",
    "\n",
    "Then using another params file where all samples are assigned to the already assembled group and specifying `--binning_map_mode all` maybe the pipeline would use the assembly as-is but map all samples to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92938c3e-340a-4c7c-8d0d-eb7e5275761d",
   "metadata": {},
   "source": [
    "For example, to run assembly for the 'mock' group:\n",
    "\n",
    "```bash\n",
    "nextflow run -c conf/custom.config -params-file conf/mag.assembly.yml nf-core/mag -r 2.3.0 -resume -profile uppmax --project snic2022-5-350 --input data/sample_list.mock-assemble.csv\n",
    "```\n",
    "\n",
    "Then to run binning for the same group:\n",
    "\n",
    "```bash\n",
    "nextflow run -c conf/custom.config -params-file conf/mag.binning.yml nf-core/mag -r 2.3.0 -resume -profile uppmax --project snic2022-5-350 --input data/sample_list.mock-binning.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e7f21-6bb1-4ace-9be1-b143c4ab7618",
   "metadata": {},
   "source": [
    "Update: This doesn't work either because the assembly is re-generated with all the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713f153-fa35-4c3d-83af-a09dbe0c3824",
   "metadata": {},
   "source": [
    "### Setting up ATLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f523a-88fa-47e2-9160-8c656572e9dd",
   "metadata": {},
   "source": [
    "As an alternative I tried [ATLAS](https://github.com/metagenome-atlas/atlas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095283e4-866d-4b47-91cb-beb74797e3de",
   "metadata": {},
   "source": [
    "Atlas is a workflow written in Snakemake that performs QC, Assembly, Binning and functional annotation of contigs using gene catalogs (genes are clustered with mmseqs and annotated with eggNOG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06bc02-4eec-4e09-938e-f59aee431795",
   "metadata": {},
   "source": [
    "To install I ran:\n",
    "\n",
    "```bash \n",
    "export CONDARC=\"/proj/snic2020-5-486/nobackup/SMS-23-6668-micegut/.condarc\"\n",
    "mamba env create -f atlas-env.yml -p envs/atlas\n",
    "```\n",
    "with the following conda environment file:\n",
    "\n",
    "```yaml\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - bioconda\n",
    "dependencies:\n",
    "  - metagenome-atlas=2.15.0\n",
    "  - pandas\n",
    "  - cookiecutter\n",
    "```\n",
    "\n",
    "As a starting point I copied the `template_config.yaml` file from the atlas package into `conf/atlas-config.yml`\n",
    "\n",
    "```bash\n",
    "cp $CONDA_PREFIX/lib/python3.10/site-packages/atlas/workflow/config/template_config.yaml conf/atlas-config.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80908767-7958-41fb-9e1d-f56d152c0e0c",
   "metadata": {},
   "source": [
    "A sample list was created with:\n",
    "\n",
    "```bash\n",
    "python src/make_sample_list.py atlas > data/sample_list_atlas.tsv\n",
    "```\n",
    "\n",
    "Set up output dir:\n",
    "\n",
    "```bash\n",
    "mkdir atlas\n",
    "cd atlas\n",
    "ln -s ../data/sample_list_atlas.tsv samples.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e62ff2-8192-4ac2-88ca-995f576d251a",
   "metadata": {},
   "source": [
    "#### Setting up cluster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d6eb5-afb8-42e5-bcab-df1a02a1209c",
   "metadata": {},
   "source": [
    "To use the cluster profile for ATLAS I ran:\n",
    "```bash\n",
    "cookiecutter --output-dir ~/.config/snakemake https://github.com/metagenome-atlas/clusterprofile.git\n",
    "```\n",
    "\n",
    "and used defaults when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a3771-2882-43ba-b9e8-64d8c8c6dbc7",
   "metadata": {},
   "source": [
    "I then updated the `~/.config/snakemake/cluster/cluster_config.yaml` file to be:\n",
    "\n",
    "```yaml\n",
    "__default__:\n",
    "  #queue: normal\n",
    "  account: \"snic2022-22-722\" # <- testing first with NBIS account\n",
    "\n",
    "rulename:\n",
    "  queue: long\n",
    "  account: \"\"\n",
    "  time_min:  # min\n",
    "  threads:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19bb4c-f501-4a96-9169-cf8d8dafd61e",
   "metadata": {},
   "source": [
    "#### Atlas QC\n",
    "\n",
    "```bash\n",
    "atlas run -w /proj/snic2020-5-486/nobackup/SMS-23-6668-micegut/atlas -c /proj/snic2020-5-486/nobackup/SMS-23-6668-micegut/conf/atlas-config.yml --profile cluster -n qc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf5582-5943-4e92-970f-6a23be76ec57",
   "metadata": {},
   "source": [
    "To test with mock samples I changed samples.tsv to only contain the `m.c`, `m.c-2` and `m.c-3` samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter] *",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
